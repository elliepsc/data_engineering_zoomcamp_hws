{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6 Homework — Apache Spark\n",
    "Data Engineering Zoomcamp 2026\n",
    "\n",
    "Dataset: Yellow Taxi November 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('homework6') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 — Spark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 — Average parquet file size\n",
    "Read November 2025 Yellow Taxi data, repartition to 4 and save as parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('data/yellow_tripdata_2025-11.parquet')\n",
    "print(f'Total rows: {df.count():,}')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(4).write.parquet('data/yellow_2025_11_repartitioned', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = 'data/yellow_2025_11_repartitioned'\n",
    "parquet_files = [f for f in os.listdir(output_dir) if f.endswith('.parquet')]\n",
    "sizes_mb = [os.path.getsize(os.path.join(output_dir, f)) / (1024 * 1024) for f in parquet_files]\n",
    "\n",
    "print(f'Number of parquet files: {len(parquet_files)}')\n",
    "print(f'File sizes (MB): {[round(s, 2) for s in sizes_mb]}')\n",
    "print(f'Average size: {sum(sizes_mb)/len(sizes_mb):.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 — Trips on November 15th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nov15 = df.filter(\n",
    "    (F.to_date(F.col('tpep_pickup_datetime')) == '2025-11-15')\n",
    ").count()\n",
    "\n",
    "print(f'Trips on November 15th: {count_nov15:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 — Longest trip in hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duration = df.withColumn(\n",
    "    'duration_hours',\n",
    "    (F.unix_timestamp('tpep_dropoff_datetime') - F.unix_timestamp('tpep_pickup_datetime')) / 3600\n",
    ")\n",
    "\n",
    "max_duration = df_duration.agg(F.max('duration_hours')).collect()[0][0]\n",
    "print(f'Longest trip: {max_duration:.1f} hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 — Spark UI port\n",
    "\n",
    "The Spark UI runs on port **4040**.\n",
    "\n",
    "Access it at: http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 — Least frequent pickup zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load zone lookup\n",
    "zones = spark.read.option('header', 'true').csv('data/taxi_zone_lookup.csv')\n",
    "zones.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count pickups per zone and join with zone names\n",
    "pickup_counts = df.groupBy('PULocationID').count()\n",
    "\n",
    "result = pickup_counts \\\n",
    "    .join(zones, pickup_counts['PULocationID'] == zones['LocationID'], 'left') \\\n",
    "    .select('Zone', 'count') \\\n",
    "    .orderBy('count') \\\n",
    "    .limit(10)\n",
    "\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
